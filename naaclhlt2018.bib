
@InProceedings{	  w13-0902,
  author	= {{Beigman Klebanov}, Beata and Flor, Michael},
  booktitle	= {Proceedings of the First Workshop on Metaphor in NLP},
  pages		= {11--20},
  publisher	= {Association for Computational Linguistics},
  title		= {{Argumentation-Relevant Metaphors in Test-Taker Essays}},
  url		= {http://www.aclweb.org/anthology/W13-0902},
  year		= {2013}
}

@Article{	  ets2:ets202331,
  author	= {Blanchard, Daniel and Tetreault, Joel and Higgins, Derrick
		  and Cahill, Aoife and Chodorow, Martin},
  title		= {{TOEFL11: A Corpus of Non-Native English}},
  journal	= {ETS Research Report Series},
  volume	= {2013},
  number	= {2},
  month		= {12},
  issn		= {2330-8516},
  url		= {http://dx.doi.org/10.1002/j.2333-8504.2013.tb02331.x},
  doi		= {10.1002/j.2333-8504.2013.tb02331.x},
  pages		= {i--15},
  year		= {2013},
  keywords	= {TOEFL{\textregistered} test, corpora, native language
		  identification}
}

@InProceedings{	  tetreault-blanchard-cahill:2013:bea,
  author	= {Tetreault, Joel and Blanchard, Daniel and Cahill, Aoife},
  title		= {A Report on the First Native Language Identification
		  Shared Task},
  booktitle	= {Proceedings of the Eighth Workshop on Innovative Use of
		  NLP for Building Educational Applications},
  month		= {6},
  pages		= {48--57},
  year		= {2013},
  address	= {Atlanta, GA, USA},
  publisher	= {Association for Computational Linguistics},
  isbn		= {9781937284473},
  url		= {http://www.aclweb.org/anthology/W13-1706}
}

@InProceedings{	  w16-1104,
  abstract	= {Automatic metaphor detection usually re-lies on various
		  features, incorporating e.g. selectional preference
		  violations or con-creteness ratings to detect metaphors in
		  text. These features rely on background corpora, hand-coded
		  rules or additional, manually created resources, all
		  specific to the language the system is being used on. We
		  present a novel approach to metaphor detection using a
		  neural network in combination with word embeddings, a
		  method that has already proven to yield promising results
		  for other natural language processing tasks. We show that
		  foregoing manual feature engineering by solely relying on
		  word embeddings trained on large corpora produces
		  comparable results to other systems, while removing the
		  need for additional resources.},
  address	= {San Diego, California},
  author	= {{Do Dinh}, Erik-L{\^{a}}n and Gurevych, Iryna},
  booktitle	= {Proceedings of the Fourth Workshop on Metaphor in NLP},
  doi		= {10.18653/v1/W16-1104},
  pages		= {28--33},
  publisher	= {Association for Computational Linguistics},
  title		= {{Token-Level Metaphor Detection using Neural Networks}},
  url		= {http://www.aclweb.org/anthology/W16-1104},
  year		= {2016}
}

@Article{	  dblp:journals/corr/bojanowskigjm16,
  title		= {{Enriching Word Vectors with Subword Information}},
  year		= {2016},
  journal	= {CoRR},
  author	= {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand
		  and Mikolov, Tomas},
  month		= {7},
  volume	= {abs/1607.04606},
  url		= {http://arxiv.org/abs/1607.04606},
  archiveprefix	= {arXiv},
  eprint	= {1607.04606}
}

@InProceedings{	  tensorflow2016,
  abstract	= {TensorFlow is a machine learning system that operates at
		  large scale and in heterogeneous environments. TensorFlow
		  uses dataflow graphs to represent computation, shared
		  state, and the operations that mutate that state. It maps
		  the nodes of a dataflowgraph across many machines in a
		  cluster, and within a machine across multiple computational
		  devices, including multicore CPUs, generalpurpose GPUs, and
		  custom-designed ASICs known as Tensor Processing Units
		  (TPUs). This architecture gives flexibility to the
		  application developer: whereas in previous "parameter
		  server" designs the management of shared state is built
		  into the system, TensorFlowenables developers to experiment
		  with novel optimizations and training algorithms.
		  TensorFlow supports a variety of applications, with a focus
		  on training and inference on deep neural networks. Several
		  Google services use TensorFlow in production, we have
		  released it as an open-source project, and it has become
		  widely used for machine learning research. In this paper,
		  we describe the TensorFlow dataflow model and demonstrate
		  the compelling performance that TensorFlow achieves for
		  several real-world applications.},
  address	= {Savannah, GA},
  author	= {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen,
		  Zhifeng and Davis, Andy and Dean, Jeffrey and Devin,
		  Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and
		  Isard, Michael and Kudlur, Manjunath and Levenberg, Josh
		  and Monga, Rajat and Moore, Sherry and Murray, Derek G and
		  Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and
		  Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng,
		  Xiaoqiang},
  booktitle	= {12th USENIX Symposium on Operating Systems Design and
		  Implementation (OSDI 16)},
  isbn		= {978-1-931971-33-1},
  pages		= {265--283},
  publisher	= {USENIX Association},
  title		= {{TensorFlow: A System for Large-Scale Machine Learning}},
  url		= {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
  year		= {2016}
}

@InProceedings{	  stemle:2016:wac-x,
  title		= {{bot.zen @ EmpiriST 2015 - A minimally-deep learning
		  PoS-tagger (trained for German CMC and Web data)}},
  author	= {Stemle, Egon W.},
  booktitle	= {Proceedings of the 10th Web as Corpus Workshop (WAC-X) and
		  the EmpiriST Shared Task},
  editor	= {Cook, Paul and Evert, Stefan and Sch{\"{a}}fer, Roland and
		  Stemle, Egon},
  month		= {8},
  pages		= {115--119},
  publisher	= {Association for Computational Linguistics},
  url		= {http://anthology.aclweb.org/W/W16/W16-2614},
  year		= {2016}
}

@InProceedings{	  baroni-dinu-kruszewski:2014:p14-1,
  title		= {{Don't count, predict! A systematic comparison of
		  context-counting vs. context-predicting semantic vectors}},
  year		= {2014},
  booktitle	= {Proceedings of the 52nd Annual Meeting of the Association
		  for Computational Linguistics (Volume 1: Long Papers)},
  author	= {Baroni, Marco and Dinu, Georgiana and Kruszewski, German},
  pages		= {238--247},
  publisher	= {Association for Computational Linguistics},
  url		= {http://www.aclweb.org/anthology/P14-1023},
  doi		= {10.3115/v1/P14-1023}
}

@Article{	  dblp:journals/corr/abs-1301-3781,
  author	= {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey
		  Dean},
  title		= {{Efficient Estimation of Word Representations in Vector
		  Space}},
  journal	= {CoRR},
  volume	= {abs/1301.3781},
  year		= {2013},
  url		= {http://arxiv.org/abs/1301.3781},
}

@Article{	  tacl570,
  author	= {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  title		= {Improving Distributional Similarity with Lessons Learned
		  from Word Embeddings},
  journal	= {Transactions of the Association for Computational
		  Linguistics},
  volume	= {3},
  year		= {2015},
  keywords	= {},
  issn		= {2307-387X},
  note		= {\url{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570}},
  pages		= {211--225}
}

@Article{	  hochreiter:1997:lsm:1246443.1246450,
  author	= {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  title		= {Long Short-Term Memory},
  journal	= {Neural Computation},
  issue_date	= {November 15, 1997},
  volume	= {9},
  number	= {8},
  month		= {11},
  year		= {1997},
  issn		= {0899-7667},
  pages		= {1735--1780},
  numpages	= {46},
  url		= {https://doi.org/10.1162/neco.1997.9.8.1735},
  doi		= {10.1162/neco.1997.9.8.1735},
  acmid		= {1246450},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA}
}

@InProceedings{	  1120431,
  author	= {Jakub{\'{i}}{\v{c}}ek, Milo{\v{s}} and Kilgarriff, Adam
		  and Kov{\'a}{\v{r}}, Vojt{\v{e}}ch and Rychl{\`{y}}, Pavel
		  and Suchomel, V{\'{i}}t},
  address	= {Lancaster},
  booktitle	= {7th International Corpus Linguistics Conference ({CL}
		  2013)},
  howpublished	= {online},
  location	= {Lancaster},
  pages		= {125-127},
  title		= {The {TenTen} Corpus Family},
  url		= {http://ucrel.lancs.ac.uk/cl2013/},
  year		= {2013}
}

@Article{	  collobert:2011b,
  title		= {{Natural Language Processing (almost) from Scratch}},
  author	= {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on
		  and Karlen, Michael and Kavukcuoglu, Koray and Kuksa,
		  Pavel},
  journal	= {Journal of Machine Learning Research},
  volume	= {12},
  pages		= {2493--2537},
  year		= {2011},
  url		= {https://arxiv.org/abs/1103.0398}
}

@Book{		  oreilly2000,
  author	= {O'Reilly, Randall C. and Munakata, Yuko},
  isbn		= {0262650541},
  pages		= {504},
  publisher	= {MIT Press},
  title		= {{Computational Explorations in Cognitive Neuroscience
		  Understanding the Mind by Simulating the Brain}},
  year		= {2000}
}

@Article{	  srivastava2014,
  abstract	= {Deep neural nets with a large number of parameters are
		  very powerful machine learning systems. However,
		  overfitting is a serious problem in such networks. Large
		  networks are also slow to use, making it difficult to deal
		  with overfitting by combining the predictions of many
		  different large neural nets at test time. Dropout is a
		  technique for addressing this problem. The key idea is to
		  randomly drop units (along with their connections) from the
		  neural network during training. This prevents units from
		  co-adapting too much. During training, dropout samples from
		  an exponential number of different “thinned” networks.
		  At test time, it is easy to approximate the effect of
		  averaging the predictions of all these thinned networks by
		  simply using a single unthinned network that has smaller
		  weights. This significantly reduces overfitting and gives
		  major improvements over other regularization methods. We
		  show that dropout improves the performance of neural
		  networks on supervised learning tasks in vision, speech
		  recognition, document classification and computational
		  biology, obtaining state-of-the-art results on many
		  benchmark data sets},
  archiveprefix	= {arXiv},
  arxivid	= {1102.4807},
  author	= {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky,
		  Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  doi		= {10.1214/12-AOS1000},
  eprint	= {1102.4807},
  isbn		= {1532-4435},
  issn		= {15337928},
  journal	= {Journal of Machine Learning Research (JMLR)},
  keywords	= {deep learning, model combination, neural
		  networks, regularization},
  pages		= {1929--1958},
  title		= {{Dropout : A Simple Way to Prevent Neural Networks from
		  Overfitting}},
  volume	= {15},
  year		= {2014}
}

@Article{	  wackycorpora2009,
  title		= {{The WaCky wide web: a collection of very large
		  linguistically processed web-crawled corpora}},
  year		= {2009},
  journal	= {Language Resources and Evaluation},
  author	= {Baroni, Marco and Bernardini, Silvia and Ferraresi,
		  Adriano and Zanchetta, Eros},
  number	= {3},
  month		= {2},
  pages		= {209--226},
  volume	= {43},
  publisher	= {Springer Netherlands},
  url		= {http://www.springerlink.com/content/c348pu7321gx5081/},
  doi		= {10.1007/s10579-009-9081-4},
  issn		= {1574-020X},
  keywords	= {Humanities, Social Sciences and Law}
}

@Article{	  arxiv:1310.4546,
  author	= {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and
		  Corrado, Greg and Dean, Jeffrey},
  title		= {{Distributed Representations of Words and Phrases and
		  their Compositionality}},
  archiveprefix	= {arXiv},
  arxivid	= {1310.4546},
  eprint	= {1310.4546},
  journal	= {CoRR},
  volume	= {abs/1310.4546},
  month		= {10},
  year		= {2013},
  url		= {http://arxiv.org/abs/1310.4546}
}

@InProceedings{	  stemle:2016:evalita,
  title		= {{bot.zen @ EVALITA 2016 - A minimally-deep learning
		  PoS-tagger (trained for Italian Tweets)}},
  year		= {2016},
  booktitle	= {Proceedings of Third Italian Conference on Computational
		  Linguistics (CLiC-it 2016) {\&} Fifth Evaluation Campaign
		  of Natural Language Processing and Speech Tools for
		  Italian. Final Workshop (EVALITA 2016)},
  author	= {Stemle, Egon W.},
  editor	= {Basile, Pierpaolo and Corazza, Anna and Cutugno, Franco
		  and Montemagni, Simonetta and Nissim, Malvina and Patti,
		  Viviana and Semeraro, Giovanni and Sprugnoli, Rachele},
  month		= {12},
  address	= {Napoli, Italy}
}

@Book{	  steen2010,
  title		= {{A Method for Linguistic Metaphor Identification: From MIP
		  to MIPVU}},
  year		= {2010},
  author	= {Steen, Gerard J. and Dorst, Aletta G. and Herrmann, J.
		  Berenike and Kaal, Anna A. and Krennmayr, Tina and Pasma,
		  Trijntje},
  pages		= {238},
  isbn		= {9789027288158},
  publisher	= {John Benjamins},
  issn		= {1566-7774}
}

@Book{		  lakoffjohnson80,
  title		= {{Metaphors we Live by}},
  year		= {1980},
  author	= {Lakoff, George and Johnson, Mark},
  publisher	= {University of Chicago Press},
  isbn		= {978-0-226-46800-6},
  keywords	= {cognitive science, language processing, semantic
		  knowledge}
}

@Article{	  doi:10.1080/10926480709336752,
  title		= {{MIP: A Method for Identifying Metaphorically Used Words
		  in Discourse}},
  year		= {2007},
  journal	= {Metaphor and Symbol},
  author	= {{"Pragglejaz Group"}},
  number	= {1},
  month		= {1},
  pages		= {1--39},
  volume	= {22},
  publisher	= {Routledge},
  url		= {https://www.tandfonline.com/doi/abs/10.1080/10926480709336752
		  http://www.tandfonline.com/doi/abs/10.1080/10926480709336752},
  doi		= {10.1080/10926480709336752},
  issn		= {1092-6488}
}

@PhDThesis{	  grady1997,
  title		= {{Foundations of Meaning: Primary Metaphors and Primary
		  Scenes}},
  year		= {1997},
  author	= {Grady, Joseph},
  school	= {University of California, Berkeley}
}

@InProceedings{	  w16-1100,
  title		= {{Proceedings of the Fourth Workshop on Metaphor in NLP}},
  year		= {2016},
  booktitle	= {Proceedings of the Fourth Workshop on Metaphor in NLP},
  author	= {Klebanov, Beata Beigman and Shutova, Ekaterina and
		  Lichtenstein, Patricia},
  publisher	= {Association for Computational Linguistics},
  url		= {http://www.aclweb.org/anthology/W16-1100}
}

@InProceedings{	  w15-1400,
  title		= {{Proceedings of the Third Workshop on Metaphor in NLP}},
  year		= {2015},
  booktitle	= {Proceedings of the Third Workshop on Metaphor in NLP},
  author	= {Shutova, Ekaterina and Klebanov, Beata Beigman and
		  Lichtenstein, Patricia},
  publisher	= {Association for Computational Linguistics},
  url		= {http://www.aclweb.org/anthology/W15-1400}
}

@InProceedings{	  leong2018,
  title		= {{A report on the 2018 VUA metaphor detection shared
		  task}},
  year		= {2018},
  booktitle	= {Proceedings of the Workshop on Figurative Language
		  Processing},
  author	= {Leong, Chee Wee and Beigman Klebanov, Beata and Shutova,
		  Ekaterina},
  month		= {6},
  address	= {New Orleans, LA}
}

@Article{	  schuster1997,
  title		= {{Bidirectional recurrent neural networks}},
  year		= {1997},
  journal	= {IEEE Transactions on Signal Processing},
  author	= {Schuster, M. and Paliwal, K.K.},
  number	= {11},
  pages		= {2673--2681},
  volume	= {45},
  publisher	= {IEEE Press},
  url		= {http://ieeexplore.ieee.org/document/650093/},
  doi		= {10.1109/78.650093},
  issn		= {1053587X}
}

@Article{	  kotsiantis2006,
  title		= {{Handling imbalanced datasets: A review}},
  year		= {2006},
  journal	= {GESTS International Transactions on Computer Science and
		  Engineering},
  author	= {Kotsiantis, Sotiris and Kanellopoulos, Dimitris and
		  Pintelas, Panayiotis},
  volume	= {30},
  url		= {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.9248&rep=rep1&type=pdf}
}

@Misc{		  seidlhofer2013,
  title		= {{The Vienna-Oxford International Corpus of English
		  (VOICE)}},
  year		= {2013},
  author	= {Seidlhofer, Barbara and Breiteneder, Angelika and
		  Klimpfinger, Theresa and Majewski, Stefan and
		  Osimk-Teasdale, Ruth and Pitzl, Marie-Luise and Radeka,
		  Michael}
}

@Article{	  elman1990,
  title		= {{Finding structure in time}},
  year		= {1990},
  journal	= {Cognitive Science},
  author	= {Elman, Jeffrey L.},
  number	= {2},
  pages		= {179--211},
  volume	= {14},
  isbn		= {1551-6709},
  doi		= {10.1016/0364-0213(90)90002-E},
  issn		= {03640213},
  pmid		= {19563812}
}

@Misc{		  chollet2015,
  author	= {Chollet, François},
  title		= {{Keras: Deep Learning library for Theano and TensorFlow}},
  year		= {2015},
  publisher	= {GitHub},
  journal	= {GitHub repository},
  url		= {https://github.com/fchollet/keras},
  urldate	= {2016-03-22},
  commit	= {657b9fb48e93b59083d2e0b8a5e4daf237179dbc}
}

@PhDThesis{	  hochreiter1991,
  author	= {Hochreiter, Sepp},
  school	= {TU M{\"{u}}nchen},
  title		= {{Untersuchungen zu dynamischen neuronalen Netzen}},
  type		= {diploma thesis},
  year		= {1991}
}

