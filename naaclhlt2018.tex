%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}

% Added language, encoding and TODOs support
% 201803 by egon.stemle@eurac.edu
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
% NOTES  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[usenames]{xcolor}
\usepackage[draft,inline,nomargin,index]{fixme}
\fxsetup{theme=color,mode=multiuser}

\FXRegisterAuthor{alex}{al}{\color{violet!60}Alex} 
\FXRegisterAuthor{egon}{eg}{\color{red!60}Egon} 

\newcommand{\egon}[1]{\egonnote{#1}}
\newcommand{\alex}[1]{\alexnote{#1}}
% table packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{siunitx}
\sisetup{input-ignore={,},group-separator={,},input-decimal-markers={.}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}
% Added
% 201804 by egon.stemle@eurac.edu
% 'Solve' error: \pdfendlink ended up in different nesting level than \pd %%%%%
%\let\oldhref\href
%\renewcommand{\href}[2]{\oldhref{#1}{\hbox{#2}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\usepackage{xspace}
\newcommand\fT{\texttt{fastText}\xspace}


\title{Using Language Learner Data for Metaphor Detection}

\author{Egon W.~Stemle \\
  Eurac Research \\
  Bolzano-Bozen, Italy \\
  {\tt egon.stemle@eurac.edu} \\\And
  Alexander Onysko \\
  Alpen-Adria-Universität \\
  Klagenfurt a.W., Austria \\
  {\tt alexander.onysko@aau.at} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This article describes the system that participated in the shared task (ST) on metaphor detection \cite{Leong2018ATask} on the VU Amsterdam Metaphor Corpus (VUA).
The ST was part of the workshop on processing figurative language at the 16th annual conference of the \emph{North American Chapter of the Association for Computational Linguistics} (NAACL2018).

The system combines a small assertion of trending techniques, which implement matured methods, from NLP and ML; 
in particular, the system uses word embeddings from standard corpora and from corpora representing different proficiency levels of language learners in a LSTM BRNN architecture.

The system is available under the APLv2 open-source license.
\end{abstract}


\section{Introduction} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:intro}
 
Ever since conceptual metaphor theory was laid out in \newcite{LakoffJohnson80}, the most vexing question has remained a methodological one: how can conceptual metaphors be reliably identified in language use? Although manual identification was put on a stronger methodological footing with the Metaphor Identification Procedure (MIP) \cite{doi:10.1080/10926480709336752} and its elaboration into MIPVU \cite{Steen2010}, fuzzy areas remain due to the fact that conceptual metaphors can vary between primary metaphors and complex metaphors \cite[cf.][]{Grady1997}. 
Furthermore, highly conventionalized metaphorical expressions might not be processed in the same way as novel metaphors. The core process of manual metaphor identification is not completely unproblematic either since it can be difficult to establish whether the meaning of a lexical unit in its context deviates from its basic meaning or not. 
In the face of that slippery terrain, automatic metaphor identification emerges as an extremely challenging task. 
An increasing volume of research since the start of annual workshops at NAACL in 2013 has shown first promising results using different methods of automated metaphor identification \cite[cf.][]{W13-0900,W14-2300,W15-1400,W16-1100}.
The current shared task of metaphor identification provided a further opportunity to put the computational spotting of metaphors to the test.

Our bid for this task combines \fT \cite{bojanowski2016enriching} word embeddings (WEs) with a single-layer Long Short Term Memory (LSTM) bidirectional recurrent neural network (BRNN) architecture.
The input, parallel sequences of unlabelled \fT representations of words, is fed into the BRNN which in turn predicts metaphorical usage for each word. 
The WEs were learnt on different large corpora (BNC, Wikipedia, enTenTen) and the TOEFL11 Corpus of Non-Native English \cite{ETS2:ETS202331}; a corpus that was used, among others, in the First Native Language Identification Shared Task \cite{tetreault-blanchard-cahill:2013:BEA} held at the \emph{8th Workshop on Innovative Use of NLP for Building Educational Applications} as part of NAACL-HLT 2013.

We were lead by the idea that metaphorical language use changes while gaining proficiency in a language, and so we hoped to be able to utilise the information contained in corpora of different proficiency levels.

The paper is organised as follows: We present our system design with related work in
Section~\ref{sec:design}, the implementation in
Section~\ref{sec:implementation}, and its evaluation in
Section~\ref{sec:results}. 
Section~\ref{sec:conclusion} concludes with an outlook on possible
implementation improvements.



\section{Design} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:design}

Generally, our design builds upon the foundation laid out by~\newcite{collobert:2011b} for a neural network (NN) architecture and learning algorithm that can be applied to various natural language processing tasks.
The most related task specific design is likely \newcite{W16-1104} who used a NN in combination with WEs to detect metaphors. 
% they demonstrate positive influence of part-of-speech (POS) features.
But they used a dense multi-layer NN, and we adapted the design of \newcite{stemle:2016:WAC-X,stemle:2016:evalita} who combined WEs with a recurrent NN (RNN) to predict part-of-speech (PoS) tags of computer-mediated communication (CMC) and Web corpora for German and Italian.
RNNs are usually considered to be more suitable for labelling sequential data such as text.


\subsection{Word Embeddings} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{subsec:we}

Recently, state-of-the-art results on various linguistic tasks were accomplished by architectures using neural-network based WEs.
\newcite{baroni-dinu-kruszewski:2014:P14-1} conducted a set of experiments  comparing the popular word2vec~\cite{DBLP:journals/corr/abs-1301-3781,arXiv:1310.4546} implementation for creating WEs with other distributional methods that yielded valuable results across various (semantic) tasks. 
These results suggest that the word embeddings substantially outperform the other architectures on semantic similarity and analogy detection tasks.
Subsequently,~\newcite{TACL570} conducted a comprehensive set of experiments and comparisons that suggest that much of the improved results are due to  the system design and parameter optimizations, rather than the selected method.  
They conclude that "there does not seem to be a consistent significant advantage to one approach over the other".

Word embeddings provide high-quality low dimensional vector representations of words from large corpora of unlabelled data. The representations, typically computed using NNs, encode many linguistic regularities and patterns~\cite{arXiv:1310.4546}.


\subsection{Bidirectional Recurrent Neural Network}  %%%%%%%%%%%%%%%%%%%%%%%%%%
\label{subsec:nn}

NNs consist of a large number of simple, highly interconnected processing nodes in an architecture loosely inspired by the structure of the cerebral cortex of the brain~\cite{oreilly2000}.
The nodes receive weighted inputs through their connections on one side and \emph{fire} according to their individual thresholds of their shared activation function.
A firing node passes on an activation to all connected successive nodes on the other side.
During learning the input is propagated through the network and the actual output is compared to the desired output. 
Then, the weights of the connections (and the thresholds) are adjusted step-wise so as to more closely resemble a configuration that would produce the desired output.
After all training data have been presented, the process typically starts over, and the learned output values will usually be closer to the desired values.

Recurrent NNs (RNNs) are NNs where the connections between the elements are directed cycles, i.e.~the networks have loops, and this enables the NN to model sequential dependencies of the input.
However, regular RNNs have fundamental difficulties learning long-term dependencies, and special kinds of RNNs need to be used~\cite{Hochreiter1991}; 
a very popular kind is the so called long short-term memory (LSTM) network proposed by~\newcite{Hochreiter:1997:LSM:1246443.1246450}.

Bidirectional RNNs (BRNN) \cite{Schuster1997BidirectionalNetworks} extend unidirectional RNNs by introducing a layer, where the directed cycles enable the input to flow in opposite sequential order. 
While processing text, this means that for any given word the network not only considers the text leading up to the word but also the text thereafter.

Overall, we benefit from available labelled data with this design but
also from large amounts of available unlabelled data.


\section{Implementation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:implementation}

We maintain the implementation in a source code repository at
\url{https://github.com/bot-zen/}.  

Our system uses sequences of word features as input to a BRNN with a LSTM architecture.
For multiple feature sets, e.g.~multiple WE models or additional PoS tags, sequences are concatenated on the word level such that the number of features for an individual word grows but the sequence length stays constant.
%\footnote{For an overview of combining multiple WE models see, e.g.~\newcite{Turian:2010:WRS:1858681.1858721}.}
Input sequences represent original textual sentence segments of an exactly pre-defined length, and in case the sentence segment is shorter than the sequence, the remaining slots are padded, i.e.~they are filled with identical dummy information.
The output is a single sequence of matching length with labels indicating whether the corresponding word is used metaphorically or not.


\subsection{Word Embeddings} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We use gensim\footnote{\url{https://radimrehurek.com/gensim/}}, a Python tool for unsupervised semantic modelling from plain text, to load pre-computed WE models, and to compute a word's embedding vector representation for our {NN}.
Words missing in a WE model, i.e.~out-of-vocabulary words (OOV), are first estimated by a fixed context of non-OOVs of the OOV word.
And if this fails, OOVs are mapped to an ultimately stable, but at first randomly generated, vector representation.


\subsection{Recurrent Neural Network Layer} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our implementation uses Keras, a high-level NNs library, written in Python and
capable of running on top of either TensorFlow~\cite{tensorflow2016} or Theano~\cite{chollet2015}. 
In our case, it runs on top of TensorFlow, an open source software library for numerical computation.

The input to our network are parallel sequences of the same length. They consist of (insert length) sentences.
\egon{formalise this} %like:
% $S_{1,1}\ ^\frown S_{1,2}, S_{2,1}\ ^\frown S_{2,2}, \ldots, S_{n,m}$
% Each $S_{n,m}$ is output of an embeddings lookup for a specific model.


\fxnote{are the input sentences those taken from the MIPVU corpus to be annotated for metaphorical words or are they extracted from the corpora (i.e. Word Embeddings)? --perhaps we could add some more information on the nature of these sentences}



\egon{need more details about "How the Word Embeddings from the different corpora are combined to make one representation? (averaging, concatenation, ...)".
}



%During training, we group sentences of the same length into batches and process
%the batches according to sentence length in increasing order.
Each single word in the sequence is represented by multiple WEs that come from different corpora. %sources (see Section~\ref{sec:results}).
%Note that the WEs of the domain-specific corpus may have only been derived
%from a small corpus.
For unknown words, i.e.~words without a pre-computed WE, we first try to
find the most similar WE considering %FIXME: 10?
surrounding words.  
If this fails, the unknown word is mapped to a randomly generated stable vector
representation.
%In Total, each word is represented by $2,280$ features: two times $500$ (WEs),
%and sixteen times $80$ for two 8-grams (word beginning and ending).
%If words are shorter than 8 characters their 8-grams are zero-padded.

This sequential input is fed into a LSTM layer that, in turn, projects to a
fully connected output layer with softmax activation function.
During training we use dropout for the projection into the output layer,
%i.e.~we set a fraction ($0.5$) of the input units to 0 at each update, 
which helps prevent overfitting~\cite{Srivastava2014}.
We use a weighted categorical cross-entropy loss function \cite{} and backpropagation in
conjunction with the RMSprop optimization \cite{} for learning.  The weights are calculated according to the logarithm of the inverse count of metaphorical vs.~non-metaphorical words.
At the time of writing, this was the Keras default---or the explicitly
documented and suggested option to be used---for our type of architecture. 

\fxnote{should we say something more on the “surrounding” from which WE were computed?}

\section{Experiments and Results} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:results}

\egon{Specify the predefined length of input sentences}


\egon{Mention which task(s) we participated in, and discuss all relevant results.}

\subsection{Data} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We used pre-built WEs for the following large corpora: BNC and enTenTen \cite{1120431}
from the SketchEngine\footnote{\url{https://sketchengine.co.uk}}
\cite{DBLP:journals/corr/BojanowskiGJM16} as well as Wikipedia from fastText
\cite{mikolov2018advances}. 

The considerably smaller TOEFL11 corpus was split into three parts corresponding to the three labelled proficiency levels (low, medium, high) and WEs were computed for each.
\egon{more on how the WE were computed from TOEFL 11}

\subsubsection{Language Learner Data} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our experimental design also utilizes data from language learner corpora, specifically from the TOEFL 11 corpus \cite{ETS2:ETS202331}. This is based on the intuition that metaphor use might vary depending on learner proficiency.
\newcite{W13-0902} %p.18 
indeed found a correlation between higher proficiency ratings of learner texts and a higher density of metaphors in these texts. Their study is also one of the few in the field of automated metaphor detection that are concerned with learner language. Their aim, however, is quite different to the current study as they try to establish annotations for metaphoric language use that can help to train an automated classifier of metaphors in test-taker essays. The current study, by contrast, utilizes learner corpus data to build WEs among other corpora representing written standard language. Learner language could be a particularly helpful source of information for automated metaphor detection via WEs as learner language provides different usage patterns compared to WEs derived from standard language corpora.


\subsubsection{Pre-trained Word Embedding Models}


\subsubsection{Overview of all Input}

%%\begin{table*}[t]
%\begin{table}[!ht]
%\begin{center}
%\small
%\begin{tabular}{|l|r|r|r|}
%\hline
%\multicolumn{1}{|c}{\textbf{Name}}		& 
%\multicolumn{1}{c}{\shortstack[c]{\textbf{Tokens} \\ \textbf{(Mio)}}} 		& 
%\multicolumn{1}{c}{\shortstack[c]{\textbf{min} \\ \textbf{Cnt}}} 			& 
%\multicolumn{1}{c|}{\textbf{dim}} 		\\ 
%\hline
%\hline
%% https://embeddings.sketchengine.co.uk/static/index.html
%BNC   		& $100$ 	& 5		& 100 \\ % BNC 100-million-word, 100.000.000
%\hline
%enTenTen13	& $20,000$ 	& 5		& 100 \\ %
%\hline
%\hline
%% https://fasttext.cc/docs/en/pretrained-vectors.html
%Wikipedia17	& $~2,300$  & 5	& 300 \\ %
%\hline
%\hline
%ukWaC 		& $2,100$ 	& 5	& 100 \\ % 2.119.891.296
%ukWaC T11-size	& $3.5$ & 1	&  50 \\ %
%\hline
%%\multicolumn{4}{|l|}{TOEFL11 (T11) Training Set} \\
%%% https://catalog.ldc.upenn.edu/LDC2014T06
%%\hline
%T11 (low) 	& $0.3$ 	& 1	&  50 \\ %   245.130
%T11 (med) & $1.8$	& 1	&  50 \\ % 1.819.407
%T11 (high)	& $1.4$ 	& 1	&  50 \\ % 1.388.260
%%\hline
%T11 (low+med+high)	& $3.5$ 	& 1	&  50 \\ % 3.452.797 tokens
%\hline
%VOICE       & $1$ 	 	& 1	&  50  \\ % Voice 1-million-word, 1.003.696
%\hline
%\end{tabular}
%\end{center}
%\caption{\label{tab:models}.} 
%\end{table}
%%\end{table*}
%

\begin{table*}[t]
\setlength{\tabcolsep}{3pt}
\begin{center}
%\small
\begin{tabular}{|l|S[table-format=6.1]|r|r||c|c|c|c|c|c|c|c|c|c||r|rr|}
\hline
    \multicolumn{1}{|c}{}		& 
    \multicolumn{1}{|c| }{\shortstack[c]{\textbf{Tokens} \\ \textbf{(Mio)}}}		& 
    \multicolumn{1}{ c| }{\shortstack[c]{\textbf{min} \\ \textbf{Cnt}}}		& 
    \multicolumn{1}{ c||}{\shortstack[c]{\textbf{dim} }}		& 
    \multicolumn{1}{ c| }{\rotatebox{90}{T11 (low)}} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{T11 (med)}} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{T11 (high)}} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{T11 (l+m+h)}} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{VOICE}} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{BNC}} 		& 
    \multicolumn{1}{ c| }{\rotatebox{90}{enTenTen13}} 			& 
    \multicolumn{1}{ c| }{\rotatebox{90}{ukWaC}} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{ukWaC T11-size\ }} 		&
    \multicolumn{1}{ c||}{\rotatebox{90}{Wikipedia17}} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{F-score on Test Set\ }} 		&
    \multicolumn{2}{ c| }{\shortstack[c]{10-fold CV \\ Accuracy \\ on Training Set \\ $\mu$ | $\sigma$ }} 		\\
\hline
\hline


%T11 (low) 	& $0.3$ 	& 1	&  50 \\ %   245.130
%T11 (med)  & $1.8$	    & 1	&  50 \\ % 1.819.407
%T11 (high)	& $1.4$ 	& 1	&  50 \\ % 1.388.260
%%\hline
%T11 (low+med+high)	& $3.5$ 	& 1	&  50 \\ % 3.452.797 tokens

\multirow[t]{2}{*}{T11 (low)} % 245.130
    % NLI_2013_low_min1_dim50-10fold    0.207   91.7    +/- 1.64
    & 0.3 & 1 & 50 & X &   &   &  &  & &  &  &  & & 0.207 & 0.917 & 0.016  \\
    \cline{2-17}
    % NLIs_low_med_high__voice__bnc-10fold  0.601   95.0    +/- 0.36
    & 204.5 &  &   & X  & X & X  &  & X  & X &  &  &  &  & 0.601  & 0.950 & 0.004   \\
    \cline{2-17}
    % NLIs_low_med_high__voice__ententen-10fold 0.603   94.7    +/- 0.64
    & 19,004.5  &  &   & X  & X & X  &  & X  &  & X &  &  &  & 0.603  & 0.947 & 0.006   \\
    \cline{2-17}
    % NLIs_low_med_high__voice__bnc__wiki__ententen-10fold  0.597   95.2    +/- 0.31
    & 21,404.5  &  &   & X  & X & X  &  & X  & X & X &  &  & X & 0.597  & \textbf{0.952} & 0.003   \\
    \cline{2-17}
    % NLIs_low_med_high__ukwac-NLI-size_sample_min1_dim50-10fold    0.576   94.1    +/- 0.34
    & 7  &  &   & X  & X & X  &  &   &  &  &  & X &  & 0.576  & 0.941 & 0.003   \\
\hline
\multirow[t]{2}{*}{T11 (med)} %  1.819.407
    % NLI_2013_med_min1_dim50-10fold    0.526   92.4    +/- 1.06
    & 1.8 & 1 & 50 &   & X &   &  &  & &  &  &  & & 0.526 & 0.924 & 0.011  \\
\hline
\multirow[t]{3}{*}{T11 (high)} %  1.388.260
    % NLI_2013_high_min1_dim50-10fold   0.514   93.0    +/- 0.74
    & 1.4 & 1 & 50 &   &   & X &   &  &  &  &  &  & & 0.514 & 0.930 & 0.007  \\
\hline
\multirow[t]{2}{*}{T11 (l+m+h) } %  3.452.797
    % NLI_2013_all_min1_dim50-10fold    0.541   92.8    +/- 0.82
    &   3.5 & 1 & 50 &  &  &   & X &   &   &  &  &  & & 0.541 & 0.928 & 0.008  \\
    \cline{2-17}
    % NLIs_all__bnc_min1_dim50-10fold   0.613   94.5    +/- 0.53
    & 103.5 &   &    &  &  &   & X &   & X &  &  &  & & \textbf{0.613} & 0.945 & 0.005  \\
\hline
\multirow[t]{2}{*}{VOICE} %  Voice 1-million-word, 1.003.696
    % voice_min1_dim50-10fold   0.495   92.3    +/- 0.95
    & 1 & 1 & 50 &  &  &  &  & X & &  &  &  & & 0.495 & 0.923 & 0.010  \\
\hline
\hline
% https://embeddings.sketchengine.co.uk/static/index.html
\multirow[t]{2}{*}{BNC} %  BNC 100-million-word, 100.000.000
    % bnc-10fold    0.597   94.2    +/- 0.54
    & 100      & 5 & 100 &  &  &  &  &   & X &    &  &  &   & 0.597 & 0.942 & 0.005  \\
    \cline{2-17}
    % ukwac-NLI-size__bnc_min1_dim50-10fold 0.597   94.8    +/- 0.31
    & 103.5    &   &     &  &  &   &  &   & X &  &  & X &  & 0.597 & 0.948 & 0.003  \\
    \cline{2-17}
    % bnc__wiki__ententen-10fold    0.605   95.1    +/- 0.34
    & 21,400 &   &     &  &  &   &  &   & X & X  &  &  & X & 0.605 & 0.951 & 0.003  \\
\hline
\multirow[t]{2}{*}{enTenTen13} % 
    % ententen-10fold   0.594   94.7    +/- 0.40
    & 19,000 & 5 & 100 & &  &  &  &  &   & X &  &  & & 0.594 & 0.947 & 0.004  \\
\hline
\hline
\multirow[t]{2}{*}{ukWaC} %  2.119.891.296 tokens
    % ukwac-10fold  0.598   94.5    +/- 0.41
    & 2,100 & 5 & 100 &  &  &  &  &  &  &  & X  &  &  & 0.598 & 0.945 & 0.004  \\
\hline % \cline{2-17}
\multirow[t]{2}{*}{ukWaC T11-size} %  
    % ukwac-NLI-size_sample_min1_dim50-10fold   0.564   93.3    +/- 0.93
    & 3.5 & 1 & 50 &  &  &  &  &  &  &  &  & X  &  & 0.564 & 0.933 & 0.009  \\
\hline
\hline
%% https://fasttext.cc/docs/en/pretrained-vectors.html
\multirow[t]{2}{*}{Wikipedia17} %  
    % wiki-10fold   0.586   94.7    +/- 0.32
    & ca 2,300 & 5 & 300 &  &  &  &  &  &  &  &  &  & X & 0.586 & 0.947 & 0.003  \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:models}.} 
\end{table*}




\subsection{Hyper-parameter tuning} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Hyper-parameter tuning is important for good performance. The parameters of our system were optimised via an ad-hoc grid search. Parameters were:
optimizer, recurrent\_dropout, sequence length, learning epochs and batch size, and the network architecture, e.g.~introducing a second LSTM abstraction layer or using a Gated Recurrent (GRU) layer instead of the LSTM layer.

\egon{describe hyper-parameters for submitted final model}

\egon{context for OOV words: 10 - never grid-searched}

We then used the same configuration for all experiments.

\subsection{Discussion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{l|r|r}
%\hline
%only bnc   		& $0.56$ \\ \hline
%only wikipedia 	& $0.58$ \\ \hline
%only enTenTen	& $0.59$ \\ \hline
%\hline
%T11 low 		& $0.57$ \\ \hline
%T11 medium		& $0.48$ \\ \hline
%T11 high		& $0.51$ \\ \hline
%\hline
%bnc + T11 (all) & $0.61$ \\ \hline
%\end{tabular}
%\end{center}
%\caption{\label{tab:results}Results with different data set, calculated via 3-fold cross validation on the training set.} 
%\end{table}

\egon{Mention evaluation method/metric/etc.}

\section{Conclusion \& Outlook} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:conclusion}
\fxnote{Add content}

Note that WEs could also be computed in an input layer. There, the input would be the actual word sequences, and the input could be accompanied by additional meta data.



\section*{Acknowledgments} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The computational results presented have been achieved in part using the \href{http://vsc.ac.at}{Vienna
Scientific Cluster (VSC)}.


\bibliographystyle{acl_natbib}
\bibliography{naaclhlt2018,Mendeley-egon}

\end{document}
