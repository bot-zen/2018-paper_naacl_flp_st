%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}

% Added language and encoding support
% 201803 by egon.stemle@eurac.edu
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

% \aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\usepackage{xspace}
\newcommand\fT{\texttt{fastText}\xspace}


\title{Using Language Learner Data for Metaphor Detection}

\author{Egon W.~Stemle \\
  Eurac Research \\ 
  Bolzano-Bozen, Italy \\
  {\tt egon.stemle@eurac.edu} \\\And
  Alexander Onysko \\
  Alpen-Adria-Universit√§t \\
  Klagenfurt a.W., Austria \\
  {\tt alexander.onysko@aau.at} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This article describes the system that participated in the shared task on metaphor detection on the VU Amsterdam Metaphor Corpus (VUA) held at the workshop on processing figurative language as part of the 16th annual conference of the North American Chapter of the Association for Computational Linguistic (NAACL2018).

The system combines a small assertion of trending techniques, which implement matured methods, from NLP and ML; in particular, the system uses word embeddings from standard corpora and from corpora representing different proficiency levels of language learners in a LSTM RNN architecture.

The system is available under the APLv2 open-source license.

NOTE: this is a draft --- we haven't received an answer to our question to the organisers whether this publication would get included in the proceedings without participation in the workshop. Given this uncertainty, the current state of the contribution mostly suffices to confirm our results.

\end{abstract}


\section{Introduction} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:intro}

Our system combines \fT \cite{bojanowski2016enriching} word embeddings (WEs) with a
single-layer Long Short Term Memory (LSTM) bidirectional recurrent neural network (BRNN)
architecture.
The input, parallel sequences of unlabelled \fT representations of words, is fed into the
BRNN which in turn predicts metaphorical usage for each word. The WEs were learnt on different large corpora (BNC, Wikipedia, enTenTen) and the TOEFL11 Corpus of Non-Native English\cite{ETS2:ETS202331} that was used, among others, in the First Native Language Identification Shared Task\cite{tetreault-blanchard-cahill:2013:BEA}
 held at the 8th Workshop on Innovative Use of NLP for Building Educational Applications as part of NAACL-HLT 2013.

We were lead by the idea that metaphorical language use changes while gaining proficiency in a language, and so we hoped to be able to utilise the information contained in corpora of different proficiency levels.

The paper is organised as follows: We present our system design in
Section~\ref{sec:design}, the implementation in
Section~\ref{sec:implementation}, and its evaluation in
Section~\ref{sec:results}. 
Section~\ref{sec:conclusion} concludes with an outlook on possible
implementation improvements.

\section{Design} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:design}

Overall, our design builds upon the foundation
laid down by~\newcite{collobert:2011b} for a neural network (NN) architecture
and learning algorithm that can be applied to various natural language
processing tasks, and is a variation of \cite{W16-1104} who used a neural network in combination with word embedding to detect metaphors.

\subsection{Word Embeddings} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recently, state-of-the-art results on various linguistic tasks were
accomplished by architectures using neural-network based WEs.
\newcite{baroni-dinu-kruszewski:2014:P14-1} conducted a set of
experiments comparing the popular
word2vec~\cite{DBLP:journals/corr/abs-1301-3781,arXiv:1310.4546}
implementation for creating WEs to other distributional methods with
state-of-the-art results across various (semantic) tasks. 
These results suggest that the word embeddings substantially
outperform the other architectures on semantic similarity and analogy
detection tasks.
Subsequently,~\newcite{TACL570} conducted a comprehensive set of
experiments and comparisons that suggest that much of the improved results are
due to the system design and parameter optimizations, rather than the selected
method.  
They conclude that "there does not seem to be a consistent significant
advantage to one approach over the other".

Word embeddings provide high-quality low dimensional vector representations of
words from large corpora of unlabelled data, and the representations, typically
computed using NNs, encode many linguistic regularities and
patterns~\cite{arXiv:1310.4546}.


\subsection{Language Learner Data}
% FIXME

\begin{quotation}
Patterns of language are usually perceived, learned and used as meaningful chunks that are processed as a whole, resulting in a reduced learning burden and increased fluency. The ability to comprehend and produce lexical chunks or groups of words which are commonly found together is an important part of language acquisition. This paper demonstrates how an awareness of conceptual metaphor and grouping of various words and expressions in a metaphorical chunk may improve the process of vocabulary acquisition. Since words that appear in language as a result of metaphorical extensions resemble other etymologically related words, this method may help learners in establishing mental associations and speed up learning, especially if students already know words to which new vocabulary is related. In this way, learners' prior knowledge would assist in assimilating new information by reducing burden on limited-capacity working memory. The current paper offers a specific example of implementing this approach, and demonstrates how Russian words and idiomatic expressions can be presented in metaphorical chunks to facilitate cognitively efficient learning. It suggests that a similar approach may potentially be extended to syntactic properties of words that could be explained via conceptual metaphors encoded in their origin. By exploring this connection, a metaphorical approach could also be used in teaching grammar.
\end{quotation}
\cite{Kalyuga2008}


\section{Implementation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:implementation}

We maintain the implementation in a source code repository at
\url{https://github.com/bot-zen/}.  
%The version tagged as {\tt 1.1} comprises the version that was used to generate
%the results submitted to the shared task (ST).

Our system feeds sequences of WEs into a single-layer BRNN with a LSTM architecture. The original input are sentences. These are split into sequences of a pre-defined length, and shorter sequences are padded.


\subsection{Word Embeddings} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% When computing WEs we take into consideration \newcite{TACL570}: they observed
% that one specific configuration of word2vec, namely the skip-gram model with
% negative sampling (SGNS) 
% "is a robust baseline.  While it might not be the best method for every task,
% it does not significantly underperform in any scenario. Moreover, SGNS is the
% fastest method to train, and cheapest (by far) in terms of disk space and
% memory consumption".
% Coincidentally,~\newcite{arXiv:1310.4546} also suggest to use SGNS.
% We incorporate word2vec's original C implementation for learning WEs%
% \footnote{\url{https://code.google.com/archive/p/word2vec/}}
% in an independent pre-processing step, i.e.~we pre-compute the WEs.

We use gensim%
\footnote{\url{https://radimrehurek.com/gensim/}}%
, a Python tool for unsupervised semantic modelling from plain text, to load
the pre-computed data, and to compute the vector representations of
input words for our NN.

\subsection{Recurrent Neural Network Layer} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our implementation uses Keras, a high-level NNs library, written in Python and
capable of running on top of either TensorFlow or Theano~\cite{chollet2015}. 
In our case it runs on top of TensorFlow, an open source software library for numerical computation~\cite{tensorflow2016}.

The input to our network are parallel sequences of the same length as the sentences we
process.
%During training, we group sentences of the same length into batches and process
%the batches according to sentence length in increasing order.
Each single word in the sequence is represented by multiple WEs that come from different corpora. %sources (see Section~\ref{sec:results}).
%Note that the WEs of the domain-specific corpus may have only been derived
%from a small corpus.
For unknown words, i.e.~words without a pre-computed WE, we first try to
find the most similar WE considering %FIXME: 10?
surrounding words.  
If this fails, the unknown word is mapped to a randomly generated stable vector
representation.
%In Total, each word is represented by $2,280$ features: two times $500$ (WEs),
%and sixteen times $80$ for two 8-grams (word beginning and ending).
%If words are shorter than 8 characters their 8-grams are zero-padded.

This sequential input is fed into a LSTM layer that, in turn, projects to a
fully connected output layer with softmax activation function.
During training we use dropout for the projection into the output layer,
%i.e.~we set a fraction ($0.5$) of the input units to 0 at each update, 
which helps prevent overfitting~\cite{Srivastava2014}.
We use a weighted categorical cross-entropy loss function and backpropagation in
conjunction with the RMSprop optimization for learning.  The weights are calculated according to the logarithm of the inverse of the count of metaphorical vs.~non-metaphorical words.
At the time of writing, this was the Keras default---or the explicitly
documented and suggested option to be used---for our type of architecture. 


\section{Results} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:results}

The parameters of the system were optimised via an ad-hoc grid search. Parameters were:
optimizer, recurrent\_dropout, sequence length, learning epochs and batch size, and the network architecture, e.g.~introducing a second LSTM abstraction layer or using a Gated Recurrent (GRU) layer instead of the LSTM layer.

We used pre-built WEs for the large corpora. BNC and enTenTen \cite{1120431}
from the SketchEngine\footnote{\url{https://sketchengine.co.uk}}
\cite{DBLP:journals/corr/BojanowskiGJM16}. Wikipedia from fastText
\cite{mikolov2018advances}. 

The considerably smaller TOEFL11 corpus was split into three parts corresponding to the three labelled proficiency levels (low, medium, high) and WEs were computed.

\begin{table}[h]
\begin{center}
\begin{tabular}{l|r|r}
\hline
only bnc   		& $0.56$ \\ \hline
only wikipedia 	& $0.58$ \\ \hline
only enTenTen	& $0.59$ \\ \hline
\hline
T11 low 		& $0.57$ \\ \hline
T11 medium		& $0.48$ \\ \hline
T11 high		& $0.51$ \\ \hline
\hline
bnc + T11 (all) & $0.61$ \\ \hline
\end{tabular}
\end{center}
\caption{\label{tab:results}Results with different data set, calculated via 3-fold cross validation on the training set.} 
\end{table}


\section{Conclusion \& Outlook} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:conclusion}

%\section*{Acknowledgments} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The computational results presented have been achieved in part using the Vienna
%Scientific Cluster (VSC).

\bibliographystyle{acl_natbib}
\bibliography{naaclhlt2018}

\end{document}
