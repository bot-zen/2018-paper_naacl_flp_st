%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}

% Added language, encoding and todos support
% 201803 by egon.stemle@eurac.edu
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[colorinlistoftodos]{todonotes}

\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

% \aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\usepackage{xspace}
\newcommand\fT{\texttt{fastText}\xspace}


\title{Using Language Learner Data for Metaphor Detection}

\author{Egon W.~Stemle \\
  Eurac Research \\ 
  Bolzano-Bozen, Italy \\
  {\tt egon.stemle@eurac.edu} \\\And
  Alexander Onysko \\
  Alpen-Adria-Universität \\
  Klagenfurt a.W., Austria \\
  {\tt alexander.onysko@aau.at} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This article describes the system that participated in the shared task on metaphor detection on the VU Amsterdam Metaphor Corpus (VUA) held at the workshop on processing figurative language as part of the 16th annual conference of the North American Chapter of the Association for Computational Linguistic (NAACL2018).

The system combines a small assertion of trending techniques, which implement matured methods, from NLP and ML; in particular, the system uses word embeddings from standard corpora and from corpora representing different proficiency levels of language learners in a LSTM RNN architecture.

The system is available under the APLv2 open-source license.

NOTE: this is a draft --- we haven't received an answer to our question to the organisers whether this publication would get included in the proceedings without participation in the workshop. Given this uncertainty, the current state of the contribution mostly suffices to confirm our results.

\end{abstract}


\section{Introduction%
% and Related Work%
} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:intro}

\todo[color=green]{A few words on (conceptual) metaphor and its challenge for automated detection would be good here - @alex} 

Our system combines \fT \cite{bojanowski2016enriching} word embeddings (WEs) with a
single-layer Long Short Term Memory (LSTM) bidirectional recurrent neural network (BRNN)
architecture.
The input, parallel sequences of unlabelled \fT representations of words, is fed into the
BRNN which in turn predicts metaphorical usage for each word. The WEs were learnt on different large corpora (BNC, Wikipedia, enTenTen) and the TOEFL11 Corpus of Non-Native English\cite{ETS2:ETS202331} that was used, among others, in the First Native Language Identification Shared Task\cite{tetreault-blanchard-cahill:2013:BEA}
 held at the 8th Workshop on Innovative Use of NLP for Building Educational Applications as part of NAACL-HLT 2013.

We were lead by the idea that metaphorical language use changes while gaining proficiency in a language, and so we hoped to be able to utilise the information contained in corpora of different proficiency levels.

The paper is organised as follows: We present our system design in
Section~\ref{sec:design}, the implementation in
Section~\ref{sec:implementation}, and its evaluation in
Section~\ref{sec:results}. 
Section~\ref{sec:conclusion} concludes with an outlook on possible
implementation improvements.

\section{Design} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:design}

Overall, our design builds upon the foundation
laid out by~\newcite{collobert:2011b} for a neural network (NN) architecture
and learning algorithm that can be applied to various natural language
processing tasks. The current design is a variation of \cite{W16-1104} who used a neural network in combination with word embedding to detect metaphors.

\subsection{Word Embeddings} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recently, state-of-the-art results on various linguistic tasks were
accomplished by architectures using neural-network based WEs.
\newcite{baroni-dinu-kruszewski:2014:P14-1} conducted a set of experiments comparing the popular
word2vec~\cite{DBLP:journals/corr/abs-1301-3781,arXiv:1310.4546}
implementation for creating WEs with other distributional methods that yielded
valuable results across various (semantic) tasks. 
These results suggest that the word embeddings substantially
outperform the other architectures on semantic similarity and analogy
detection tasks.
Subsequently,~\newcite{TACL570} conducted a comprehensive set of
experiments and comparisons that suggest that much of the improved results are
due to the system design and parameter optimizations, rather than the selected
method.  
They conclude that "there does not seem to be a consistent significant
advantage to one approach over the other".% FIXME? (2015: page number!).

Word embeddings provide high-quality low dimensional vector representations of
words from large corpora of unlabelled data. The representations, typically
computed using NNs, encode many linguistic regularities and
patterns~\cite{arXiv:1310.4546}.


\subsection{Language Learner Data}
% FIXME:
% write something about language learner data, e.g. why and how it might be helpful...

Our experimental design also utilizes data from language learner corpora, specifically from the TOEFL 11 corpus \cite{ETS2:ETS202331}. This is based on the intuition that metaphor use might vary depending on learner proficiency.
\newcite{W13-0902} %p.18 
indeed found a correlation between higher proficiency ratings of learner texts and a higher density of metaphors in these texts. Their study is also one of the few in the field of automated metaphor detection that are concerned with learner language. Their aim, however, is quite different to the current study as they try to establish annotations for metaphoric language use that can help to train an automated classifier of metaphors in test-taker essays. The current study, by contrast, utilizes learner corpus data to build WEs among other corpora representing written standard language. Learner language could be a particularly helpful source of information for automated metaphor detection via WEs as learner language provides different usage patterns compared to WEs derived from standard language corpora.

\todo{removed/drastically shorten loosely related quote}
\begin{quotation}
Patterns of language are usually perceived, learned and used as meaningful chunks that are processed as a whole, resulting in a reduced learning burden and increased fluency. The ability to comprehend and produce lexical chunks or groups of words which are commonly found together is an important part of language acquisition. This paper demonstrates how an awareness of conceptual metaphor and grouping of various words and expressions in a metaphorical chunk may improve the process of vocabulary acquisition. Since words that appear in language as a result of metaphorical extensions resemble other etymologically related words, this method may help learners in establishing mental associations and speed up learning, especially if students already know words to which new vocabulary is related. In this way, learners' prior knowledge would assist in assimilating new information by reducing burden on limited-capacity working memory. The current paper offers a specific example of implementing this approach, and demonstrates how Russian words and idiomatic expressions can be presented in metaphorical chunks to facilitate cognitively efficient learning. It suggests that a similar approach may potentially be extended to syntactic properties of words that could be explained via conceptual metaphors encoded in their origin. By exploring this connection, a metaphorical approach could also be used in teaching grammar.
\end{quotation}
\cite{Kalyuga2008}


\section{Implementation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:implementation}

\todo[color=green]{Specify the predefined length of input sentences. - @egon}
\todo[color=green]{a few words of explenation: "sequences are padded" - @egon}

We maintain the implementation in a source code repository at
\url{https://github.com/bot-zen/}.  
%The version tagged as {\tt 1.1} comprises the version that was used to generate
%the results submitted to the shared task (ST).

Our system feeds sequences of WEs into a single-layer BRNN with a LSTM architecture. The original input are sentences. These are split into sequences of a pre-defined length, and shorter sequences are padded.


\subsection{Word Embeddings} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo[color=green]{Add a bit of content @egon}

\todo{Should we use the/some of the following comment at all?}
% When computing WEs we take into consideration \newcite{TACL570}: they observed
% that one specific configuration of word2vec, namely the skip-gram model with
% negative sampling (SGNS) 
% "is a robust baseline.  While it might not be the best method for every task,
% it does not significantly underperform in any scenario. Moreover, SGNS is the
% fastest method to train, and cheapest (by far) in terms of disk space and
% memory consumption".
% Coincidentally,~\newcite{arXiv:1310.4546} also suggest to use SGNS.
% We incorporate word2vec's original C implementation for learning WEs%
% \footnote{\url{https://code.google.com/archive/p/word2vec/}}
% in an independent pre-processing step, i.e.~we pre-compute the WEs.

We use gensim%
\footnote{\url{https://radimrehurek.com/gensim/}}%
, a Python tool for unsupervised semantic modelling from plain text, to load
the pre-computed data, and to compute the vector representations of
input words for our Neural Network (NN).

\subsection{Recurrent Neural Network Layer} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo{are the input sentences those taken from the MIPVU corpus to be annotated for metaphorical words or are they extracted from the corpora (i.e. Word Embeddings)? --perhaps we could add some more information on the nature of these sentences}

Our implementation uses Keras, a high-level NNs library, written in Python and
capable of running on top of either TensorFlow or Theano~\cite{chollet2015}. 
In our case it runs on top of TensorFlow, an open source software library for numerical computation~\cite{tensorflow2016}.

The input to our network are parallel sequences of the same length. They consist of (insert length) sentences.
\todo[color=green]{formalise this - @egon} %like:
% $S_{1,1}\ ^\frown S_{1,2}, S_{2,1}\ ^\frown S_{2,2}, \ldots, S_{n,m}$
% Each $S_{n,m}$ is output of an embeddings lookup for a specific model.

%During training, we group sentences of the same length into batches and process
%the batches according to sentence length in increasing order.
Each single word in the sequence is represented by multiple WEs that come from different corpora. %sources (see Section~\ref{sec:results}).
%Note that the WEs of the domain-specific corpus may have only been derived
%from a small corpus.
For unknown words, i.e.~words without a pre-computed WE, we first try to
find the most similar WE considering %FIXME: 10?
surrounding words.  
If this fails, the unknown word is mapped to a randomly generated stable vector
representation.
%In Total, each word is represented by $2,280$ features: two times $500$ (WEs),
%and sixteen times $80$ for two 8-grams (word beginning and ending).
%If words are shorter than 8 characters their 8-grams are zero-padded.

This sequential input is fed into a LSTM layer that, in turn, projects to a
fully connected output layer with softmax activation function.
During training we use dropout for the projection into the output layer,
%i.e.~we set a fraction ($0.5$) of the input units to 0 at each update, 
which helps prevent overfitting~\cite{Srivastava2014}.
We use a weighted categorical cross-entropy loss function and backpropagation in
conjunction with the RMSprop optimization for learning.  The weights are calculated according to the logarithm of the inverse count of metaphorical vs.~non-metaphorical words.
At the time of writing, this was the Keras default---or the explicitly
documented and suggested option to be used---for our type of architecture. 

\todo{should we say something more on the “surrounding” from which WE were computed?}

\section{Experiments and Results} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:results}

\subsection{Dataset} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We used pre-built WEs for the following large corpora: BNC and enTenTen \cite{1120431}
from the SketchEngine\footnote{\url{https://sketchengine.co.uk}}
\cite{DBLP:journals/corr/BojanowskiGJM16} as well as Wikipedia from fastText
\cite{mikolov2018advances}. 

The considerably smaller TOEFL11 corpus was split into three parts corresponding to the three labelled proficiency levels (low, medium, high) and WEs were computed for each.
\todo[color=green]{more on how the WE were computed from TOEFL 11 - @egon}

\subsection{Hyper-parameter tuning} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Hyper-parameter tuning is important for good performance. The parameters of our system were optimised via an ad-hoc grid search. Parameters were:
optimizer, recurrent\_dropout, sequence length, learning epochs and batch size, and the network architecture, e.g.~introducing a second LSTM abstraction layer or using a Gated Recurrent (GRU) layer instead of the LSTM layer.

We then used the same configuration for all experiments.

\subsection{Discussion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\begin{center}
\begin{tabular}{l|r|r}
\hline
only bnc   		& $0.56$ \\ \hline
only wikipedia 	& $0.58$ \\ \hline
only enTenTen	& $0.59$ \\ \hline
\hline
T11 low 		& $0.57$ \\ \hline
T11 medium		& $0.48$ \\ \hline
T11 high		& $0.51$ \\ \hline
\hline
bnc + T11 (all) & $0.61$ \\ \hline
\end{tabular}
\end{center}
\caption{\label{tab:results}Results with different data set, calculated via 3-fold cross validation on the training set.} 
\end{table}


\section{Conclusion \& Outlook} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:conclusion}
\todo{Add content}

%\section*{Acknowledgments} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The computational results presented have been achieved in part using the Vienna
%Scientific Cluster (VSC).

\bibliographystyle{acl_natbib}
\bibliography{naaclhlt2018}

\end{document}
